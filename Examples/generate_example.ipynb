{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f46e24-d45d-4ea5-9581-0004ea3bbb72",
   "metadata": {},
   "source": [
    "#### This Jupyter Notebook contains example code to demonstrate how one may use the functions in biosynseq.generate to generate nucleotide sequences in fasta format and to obtain sequence embeddings from the fasta file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97fb3bc4-41d5-4866-b545-15608ed2f877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/lind/.conda/envs/mdhpipeline/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from biosynseq import generate\n",
    "from gene_transformer.config import ModelSettings\n",
    "from gene_transformer.model import LoadPTCheckpointStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e40018-4de7-4b0f-b4ce-4cc1254751b5",
   "metadata": {},
   "source": [
    "# Part 1: Get fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490f539d-beab-42c8-b41c-d71befc83ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path = \"/homes/lind/MDH-pipeline/mdh_gpt.yaml\"\n",
    "pt_path = \"/homes/mzvyagin/gpt2_mdh_example/gpt2_earnest_river_122_mdh.pt\"\n",
    "fasta_path = \"/homes/lind/MDH-pipeline/fasta/fasta_test2.fasta\"\n",
    "num_seqs = 7 # number of nucleotide sequences to generate, default = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56886313-32da-4495-8613-22fe572b441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelSettings.from_yaml(yaml_path)\n",
    "\n",
    "# given a pt file:\n",
    "model_strategy = LoadPTCheckpointStrategy(config, pt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4461447a-952e-4978-add1-b9ad3d2c06ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/lind/.conda/envs/mdhpipeline/lib/python3.9/site-packages/pytorch_lightning/core/saving.py:213: UserWarning: Found keys that are in the model state dict but not in the checkpoint: ['model.lm_head.weight']\n",
      "  rank_zero_warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unique_seqs': ['ATGCCCCTCTTGCGTGACGTGCCCCTGTGCGATATGTTATAA', 'ATGCCACAGGGTCAGTTGTCCTATGTCAAAGAGCGTCAAGAAGATATCGGTCAGCGCTAG', 'ATGCAGATGGTATAG', 'ATGCTTCGAAGCCCCTGA', 'ATGAGCTGA', 'ATGGAACTGATTCTGAAACCGCTCCCATTGGGTACGTATCATGTTAGCCCTCGGCCAATCTGA', 'ATGCTGCCCCGCATCTTGACCACTCGTTTCGGCCCGCAATCAGAAGTGCTCGGTAGACCCGGCGGTCACGGGTGGTCCGTAGTAATCTACTGA'], 'all_generated_seqs': ['ATGCCCCTCTTGCGTGACGTGCCCCTGTGCGATATGTTATAA', 'ATGCTGCCCCGCATCTTGACCACTCGTTTCGGCCCGCAATCAGAAGTGCTCGGTAGACCCGGCGGTCACGGGTGGTCCGTAGTAATCTACTGA', 'ATGCCACAGGGTCAGTTGTCCTATGTCAAAGAGCGTCAAGAAGATATCGGTCAGCGCTAG', 'ATGGAACTGATTCTGAAACCGCTCCCATTGGGTACGTATCATGTTAGCCCTCGGCCAATCTGA', 'ATGCAGATGGTATAG', 'ATGAGCTGA', 'ATGCTTCGAAGCCCCTGA'], 'seconds_elapsed': 27.565093517303467}\n"
     ]
    }
   ],
   "source": [
    "results = generate.generate_fasta(model_strategy=model_strategy, fasta_path=fasta_path, num_seqs = num_seqs)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35153162-46cf-45c4-8f1c-7fd8ea82e0b8",
   "metadata": {},
   "source": [
    "# Part 2: Get sequence embeddings from fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d54f679d-036d-433f-9031-85d935b2b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path = \"/homes/lind/MDH-pipeline/mdh_gpt.yaml\"\n",
    "pt_path = \"/homes/mzvyagin/gpt2_mdh_example/gpt2_earnest_river_122_mdh.pt\"\n",
    "fasta_path = \"/homes/lind/MDH-pipeline/fasta/fasta_test2.fasta\"\n",
    "embeddings_output_path=\"/homes/lind/MDH-pipeline/embeddings/embeddings_test.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dad02181-49cc-430b-8378-c2e5575dced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelSettings.from_yaml(yaml_path)\n",
    "\n",
    "# given a pt file:\n",
    "model_strategy = LoadPTCheckpointStrategy(config, pt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e392c5aa-e55a-4001-bf3f-930f2f7dcdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference with dataset length 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 7/7 [00:00<00:00, 13.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (7, 1024, 768)\n",
      "[[[ 0.04868069 -0.01195381 -0.00645662 ... -0.03122215  0.01094359\n",
      "   -0.0035614 ]\n",
      "  [ 0.         -0.03224799  0.         ... -0.00355151  0.02965949\n",
      "    0.00884153]\n",
      "  [ 0.02999547 -0.00865247  0.04809479 ...  0.02695912  0.05683281\n",
      "    0.01760943]\n",
      "  ...\n",
      "  [ 0.00519243  0.01638022  0.         ...  0.07288225 -0.03332997\n",
      "    0.01813556]\n",
      "  [-0.00391198  0.00714018 -0.02851456 ...  0.0271398  -0.01800548\n",
      "    0.        ]\n",
      "  [ 0.05963241 -0.01018279 -0.02210587 ... -0.06037784 -0.04219067\n",
      "    0.01604172]]\n",
      "\n",
      " [[ 0.04868069 -0.01195381 -0.00645662 ... -0.03122215  0.01094359\n",
      "   -0.0035614 ]\n",
      "  [ 0.02008249 -0.07793572 -0.0165618  ... -0.01285611  0.08464704\n",
      "   -0.02021168]\n",
      "  [ 0.01411847  0.          0.06301758 ...  0.06581816  0.04579304\n",
      "   -0.0272202 ]\n",
      "  ...\n",
      "  [ 0.00519243  0.01638022 -0.04830012 ...  0.07288225 -0.03332997\n",
      "    0.01813556]\n",
      "  [-0.00391198  0.00714018 -0.02851456 ...  0.0271398  -0.01800548\n",
      "    0.01944104]\n",
      "  [ 0.05963241 -0.01018279 -0.02210587 ... -0.06037784 -0.04219067\n",
      "    0.01604172]]\n",
      "\n",
      " [[ 0.04868069 -0.01195381 -0.00645662 ... -0.03122215  0.01094359\n",
      "   -0.0035614 ]\n",
      "  [-0.00862264  0.         -0.02136467 ...  0.03875653  0.05520017\n",
      "   -0.04015628]\n",
      "  [ 0.06632187  0.01626525  0.02366743 ...  0.02833517  0.02465641\n",
      "    0.02757922]\n",
      "  ...\n",
      "  [ 0.          0.01638022 -0.04830012 ...  0.07288225 -0.03332997\n",
      "    0.01813556]\n",
      "  [-0.00391198  0.         -0.02851456 ...  0.0271398  -0.01800548\n",
      "    0.01944104]\n",
      "  [ 0.05963241 -0.01018279 -0.02210587 ... -0.06037784  0.\n",
      "    0.01604172]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.04868069 -0.01195381 -0.00645662 ... -0.03122215  0.01094359\n",
      "   -0.0035614 ]\n",
      "  [ 0.         -0.00574316 -0.04427395 ...  0.          0.\n",
      "   -0.00841905]\n",
      "  [ 0.00894379 -0.01304797  0.00983769 ...  0.03391079  0.03693216\n",
      "   -0.00702998]\n",
      "  ...\n",
      "  [ 0.00519243  0.01638022  0.         ...  0.         -0.03332997\n",
      "    0.01813556]\n",
      "  [-0.00391198  0.         -0.02851456 ...  0.0271398  -0.01800548\n",
      "    0.01944104]\n",
      "  [ 0.05963241  0.         -0.02210587 ... -0.06037784  0.\n",
      "    0.01604172]]\n",
      "\n",
      " [[ 0.04868069 -0.01195381  0.         ... -0.03122215  0.01094359\n",
      "   -0.0035614 ]\n",
      "  [ 0.01524585  0.02895974 -0.02653178 ...  0.02567687  0.\n",
      "    0.0048639 ]\n",
      "  [ 0.01926392  0.          0.         ...  0.         -0.01571963\n",
      "    0.        ]\n",
      "  ...\n",
      "  [ 0.00519243  0.01638022 -0.04830012 ...  0.07288225 -0.03332997\n",
      "    0.01813556]\n",
      "  [ 0.          0.00714018 -0.02851456 ...  0.0271398  -0.01800548\n",
      "    0.01944104]\n",
      "  [ 0.05963241  0.         -0.02210587 ... -0.06037784 -0.04219067\n",
      "    0.01604172]]\n",
      "\n",
      " [[ 0.04868069 -0.01195381 -0.00645662 ... -0.03122215  0.01094359\n",
      "   -0.0035614 ]\n",
      "  [-0.00347719 -0.03074708 -0.00703644 ...  0.01824591 -0.0063125\n",
      "    0.0120803 ]\n",
      "  [ 0.         -0.00231808  0.07344127 ...  0.02351012  0.02025236\n",
      "    0.02177761]\n",
      "  ...\n",
      "  [ 0.00519243  0.01638022 -0.04830012 ...  0.07288225 -0.03332997\n",
      "    0.        ]\n",
      "  [-0.00391198  0.         -0.02851456 ...  0.0271398  -0.01800548\n",
      "    0.01944104]\n",
      "  [ 0.05963241 -0.01018279 -0.02210587 ... -0.06037784 -0.04219067\n",
      "    0.01604172]]]\n"
     ]
    }
   ],
   "source": [
    "embeddings = generate.fasta_to_embeddings(model_strategy, fasta_path, embeddings_output_path)\n",
    "print(embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
